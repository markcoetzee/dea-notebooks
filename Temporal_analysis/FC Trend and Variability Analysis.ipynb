{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    " **FRACTIONAL COVER: TREND AND VARIABILITY ANALYSIS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 16,
        "hidden": false,
        "row": 4,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "**<font color=green>Contents</font>**\n",
    "\n",
    "[Description and readme](#readme)\n",
    "\n",
    "[User Inputs](#userInputs)\n",
    "\n",
    "[Custom functions](#functions)\n",
    "\n",
    "1. [Pixel-wise trend and variability analysis](#pixelWiseAnalysis)\n",
    "\n",
    "    1.1 [Extract data](#get_data)\n",
    "    \n",
    "    1.2 [Masking](#masking)\n",
    "    \n",
    "    1.3 [CDO operations](#CDO_operations)\n",
    "    \n",
    "    1.4 [Statistical Tests](#welch-test)\n",
    "    \n",
    "    1.5 [Composite RGB](#rgb_plot)\n",
    "    \n",
    "    1.6 [Export geotiffs](#export_geotiffs)\n",
    "\n",
    "2. [Zonal statistics](#zonalStats)\n",
    "\n",
    "3. [Reporting and plotting](#reporting)\n",
    "\n",
    "    3.1 [Zonal timeseries plots](#timeseries)\n",
    "    \n",
    "    3.2 [Pixel-wise plots](#pixel-wise_plots)\n",
    "    \n",
    "    3.3 [True Colour plot](#truecolourplot)\n",
    "    \n",
    "    3.4 [Merge all PDFs](#mergeallpdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 41,
        "hidden": false,
        "row": 20,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "**<font color=green>Description/Readme</font>**\n",
    "<a id=\"readme\"> </a>\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "This script conducts exploratory timeseries analysis on the Fractional Cover dataset (stored in the Geoscience Australia DataCube) in order to help detect, but not attribute, change in vegetation coverage.  The script conducts the following analysis:\n",
    "\n",
    "1. Pixel-wise trend and variability analysis to produce maps of linear coefficients since some reference time, the change in seasonal and interannual variability between a baseline period and a more recent period, and the difference in mean total vegetation (PV + NPV) between a baseline period and more recent period.\n",
    "\n",
    "2. Zonal statistics for n polygons within a shapefile.\n",
    "\n",
    "3. Pixel-wise statistical tests to determine where change is significant, these test are applied to both the means (Welch T-test), and the variance (Levene test).\n",
    "\n",
    "4. Lastly, an RGB composite map is produced whose bands include: the difference in means, the difference in variability, and the welch-test probabilities.  This final map shows regions where strong regeneration from emphemeral pasture to perrenial trees is likely to have occurred.  \n",
    "\n",
    "The script relies heavily on the Climate Data Operators (CDO) library to conduct the analysis, which requires loading CDO in the terminal\n",
    "before using this script (run: module load cdo/1.7.1), and installation of the CDO python wrapper. Visit https://github.com/Try2Code/cdo-bindings for instructions on installing CDO-python through the pip installer (this only needs to be done once).\n",
    "\n",
    "The python packages PyPDF2, and rasterstats are also required and are not pre-installed on the VDI.\n",
    "\n",
    "Custom modules are also used throughout the script and are available for download from the Geoscience Australia github page https://github.com/GeoscienceAustralia/dea-notebooks/tree/master/algorithms. These include:\n",
    "\n",
    "<font color=cyan>*array_to_geotiff, rasterize_vector, load_masked_FC, dataset_to_geotiff, three_band_image, load_clearlandsat</font>*\n",
    "\n",
    "*Update:* These functions are now included at the start of the script after the user inputs section (the script is vastly longer but it doesn't require any installation of custom modules).\n",
    "\n",
    "The user should go to the 'user inputs' sections at the top of the script and enter the relevant information. Thereafter, the\n",
    "script should not need editing.  It is required that a shapefile defining the boundaries for the area of interest is supplied. The shapefile needs some preprocessing before it will run effectively, read part 2 below for more details.\n",
    "\n",
    "**Part 1:**\n",
    "\n",
    "The first part of this script conducts pixel-wise time-series analysis on the fractional cover dataset over a region defined by an input shapefile.\n",
    "\n",
    "Depending on the size of the region of interest, this script could be run on Raijin to speed up its processing. However, with Dask implemented\n",
    "it will run (almost) any size analysis through the VDI.  The script is raijin friendly so exporting it to a .py file should be fine.\n",
    "\n",
    "*Nb: Piping of CDO operations is throwing an error, so each step in the analysis is seperated. Piping speeds up the analysis so I should try to resolve this when I get a chance*\n",
    "\n",
    "**Part 2:**\n",
    "\n",
    "The second part of this script calculates the zonal mean and zonal standard deviation (std dev of the pixel values within the polygon - indication of the range of values within the polygon) of Total Vegetation (PV + NPV) over each seperate polygon in the input shapefile.\n",
    "\n",
    "**This step requires some pre-processing of the input shapefile: an attribute column uniquely identifying ecah polygon must be included. The name of the attribute column needs to be declared in the 'user inputs' section. Place this shapefile in your 'data' directory.**\n",
    "\n",
    "**Part 3:**\n",
    "\n",
    "All results are compiled into a pdf document (called 'final_results.pdf') that contains\n",
    " - A true colour map of the area of interest with the shapefile overlaying it\n",
    " - The zonal timeseries for each polygon \n",
    " - Pixel-wise plots showing: the difference between the intra-annual variability in the baseline period compared with the recent period, the         levene test p-values, the difference between the mean TV in the basline period and recent period, and Welsh T-test p values on the mean differences\n",
    " - A binary classification map showing the regions at high-risk of no-regeneration (based on the difference in variability map), and the same plot but masked just to those regions with a levene test p-value <=0.1. The binary classification map is a boolean map of 1's and 0's.  A 1 indicates areas where regeneration is unlikely to have occurred (areas where the intra-annual variability is higher in the recent period than during the baseline period, likely indicating there has been no transition from highly seasonal grasses to less climate responsive perennial vegetation (trees and shrubs).\n",
    " - And finally the RGB plot that can be used to find regions where strong regeneration is occurring (regions that are white in the map).\n",
    "\n",
    "\n",
    "\n",
    "This code was written in April/May of 2018 by Chad Burton, with significant and invaluable help from everyone in Team X and Team Wombat. The notebook was completed as a graduate program project at Geoscience Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T00:16:10.749389Z",
     "start_time": "2018-06-01T00:16:07.532864Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Bring in libraries\n",
    "import datacube\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "from datacube.storage.masking import make_mask\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import gdal\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# sys.path.append(os.path.abspath('/g/data/u46/users/cb3058/fc_changeDetection/src'))\n",
    "# sys.path.append(os.path.abspath('/g/data/u46/users/cb3058/fc_changeDetection/'))\n",
    "# from src import array_to_geotiff\n",
    "# from src import rasterize_vector\n",
    "# from src import load_nbarx\n",
    "# from src import load_masked_FC\n",
    "# from src import dataset_to_geotiff\n",
    "# from src import three_band_image\n",
    "\n",
    "import netCDF4\n",
    "from cdo import *\n",
    "cdo=Cdo()\n",
    "cdo.CDO = '/apps/cdo/1.7.1/bin/cdo'     #need to update the location of cdo or else the wrapper can't find cdo.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## USER INPUTS\n",
    "<a id=\"userInputs\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T00:16:10.781072Z",
     "start_time": "2018-06-01T00:16:10.752900Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#enter the filepath locations of the data, results, and working directory (location where you're running the script from)\n",
    "results = '/g/data/u46/users/cb3058/fc_changeDetection/results/'\n",
    "working_directory = '/g/data/u46/users/cb3058/fc_changeDetection/'\n",
    "data = working_directory + 'data/'\n",
    "\n",
    "#Enter the name of project area of interest (string)\n",
    "project_ID = \n",
    "\n",
    "#Add the filepaths for the project shapefile (string)\n",
    "shapefile_loc = data + \n",
    "\n",
    "#name of the attribute column in the shapefile with the uniquely labelled polygons (string)\n",
    "feature_name= 'FeatureNam'\n",
    "\n",
    "#start and end date of the timeseries of interest (usually shouldn't need to change this)\n",
    "start_date = '1988-01-01'\n",
    "end_date = '2017-12-31'\n",
    "\n",
    "#enter the year from which regeneration is supposed to have started \n",
    "#(i.e. often the model start date, or the date from which land-use changes begun)\n",
    "start_of_regen= '2012'\n",
    "\n",
    "#enter the baseline period (longer is better, but is often 10 years)\n",
    "start_of_baseline = '2002'\n",
    "end_of_baseline = '2011' \n",
    "\n",
    "#set the number of chunks to pass to dask \n",
    "#(bigger chunks will run the dc_load faster, but be careful of hitting memory limits)\n",
    "ds_chunks = 40\n",
    "\n",
    "## Set cloud threshold. This value defines the amount of lansdcape/cloud allowed in each scene. \n",
    "#Scenes will not be retrieved that have less than the cloud threshold worth of image.\n",
    "cloud_free_threshold = 0.80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Custom functions\n",
    "<a id=\"functions\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:41:43.399845Z",
     "start_time": "2018-06-01T03:41:41.316103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "import gdal\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "def load_clearlandsat(dc, query, masked_prop=0.99, sensors=['ls5', 'ls7', 'ls8'], mask_dict=None):\n",
    "\n",
    "    # List to save results from each sensor\n",
    "    filtered_sensors = []\n",
    "\n",
    "    # Iterate through all sensors, returning only observations with > mask_prop clear pixels\n",
    "    for sensor in sensors:\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Lazily load Landsat data using dask\n",
    "            print('Loading {} PQ'.format(sensor))\n",
    "            data = dc.load(product = '{}_nbart_albers'.format(sensor),\n",
    "                        group_by = 'solar_day', \n",
    "                        dask_chunks={'time': 1},\n",
    "                        **query)\n",
    "\n",
    "            # Remove measurements variable from query so that PQ load doesn't fail\n",
    "            pq_query = query.copy()\n",
    "            if 'measurements' in pq_query: del pq_query['measurements']\n",
    "\n",
    "            # Load PQ data\n",
    "            pq = dc.load(product = '{}_pq_albers'.format(sensor),\n",
    "                         group_by = 'solar_day',\n",
    "                         fuse_func=ga_pq_fuser,\n",
    "                         **pq_query)\n",
    "\n",
    "            # Return only Landsat observations that have matching PQ data (this may\n",
    "            # need to be improved, but seems to work in most cases)\n",
    "            data = data.sel(time = pq.time, method='nearest')\n",
    "            \n",
    "            # If a custom dict is provided for mask_dict, use these values to make mask from PQ\n",
    "            if mask_dict:\n",
    "                \n",
    "                # Mask PQ using custom values by unpacking mask_dict **kwarg\n",
    "                good_quality = masking.make_mask(pq.pixelquality, **mask_dict)\n",
    "                \n",
    "            else:\n",
    "\n",
    "                # Identify pixels with no clouds in either ACCA for Fmask\n",
    "                good_quality = masking.make_mask(pq.pixelquality,\n",
    "                                                 cloud_acca='no_cloud',\n",
    "                                                 cloud_fmask='no_cloud',\n",
    "                                                 contiguous=True)\n",
    "\n",
    "            # Compute good data for each observation as a percentage of total array pixels\n",
    "            data_perc = good_quality.sum(dim=['x', 'y']) / (good_quality.shape[1] * good_quality.shape[2])\n",
    "            \n",
    "            # Add data_perc data to Landsat dataset as a new xarray variable\n",
    "            data['data_perc'] = xr.DataArray(data_perc, [('time', data.time)])\n",
    "\n",
    "            # Filter and finally import data using dask\n",
    "            filtered = data.where(data.data_perc >= masked_prop, drop=True)\n",
    "            print('    Loading {} filtered {} timesteps'.format(len(filtered.time), sensor))\n",
    "            filtered = filtered.compute()\n",
    "            \n",
    "            # Append result to list\n",
    "            filtered_sensors.append(filtered)\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            # If there is no data for sensor or if another error occurs:\n",
    "            print('    Skipping {}'.format(sensor))\n",
    "\n",
    "    # Concatenate all sensors into one big xarray dataset, and then sort by time\n",
    "    print('Combining and sorting ls5, ls7 and ls8 data')\n",
    "    combined_ds = xr.concat(filtered_sensors, dim='time')\n",
    "    combined_ds = combined_ds.sortby('time')\n",
    "    \n",
    "    # Return combined dataset\n",
    "    return combined_ds\n",
    "\n",
    "\n",
    "\n",
    "import datacube\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "from datacube.storage.masking import make_mask\n",
    "\n",
    "dc = datacube.Datacube(app='fc_fun')\n",
    "\n",
    "def load_masked_FC(sensor, query, cloud_free_threshold):   \n",
    "    #print('loading {}'.format(sensor)\n",
    "    \n",
    "    basic_pq_mask = {'cloud_acca':'no_cloud',\n",
    "    'cloud_shadow_acca' :'no_cloud_shadow',\n",
    "    'cloud_shadow_fmask' : 'no_cloud_shadow',\n",
    "    'cloud_fmask' :'no_cloud',\n",
    "    'blue_saturated' : False,\n",
    "    'green_saturated' : False,\n",
    "    'red_saturated' : False,\n",
    "    'nir_saturated' : False,\n",
    "    'swir1_saturated' : False,\n",
    "    'swir2_saturated' : False,\n",
    "    'contiguous':True,\n",
    "    'land_sea': 'land'}\n",
    "\n",
    "    # load FC and PQ\n",
    "    fc = dc.load(product=(sensor + '_fc_albers'), group_by='solar_day', **query)\n",
    "    pq = dc.load(product=(sensor + '_pq_albers'), group_by='solar_day', **query, fuse_func=ga_pq_fuser)\n",
    "\n",
    "    crs = fc.crs\n",
    "    crswkt = fc.crs.wkt\n",
    "    affine = fc.affine\n",
    "\n",
    "    # find common observations\n",
    "    time = (fc.time - pq.time).time  # works!\n",
    "    fc = fc.sel(time=time)\n",
    "    pq = pq.sel(time=time)\n",
    "\n",
    "    # mask\n",
    "    basic_mask = make_mask(pq, **basic_pq_mask).pixelquality\n",
    "    fc = fc.where(basic_mask)    \n",
    "    cloud_free = make_mask(pq, cloud_acca='no_cloud', cloud_fmask='no_cloud').pixelquality\n",
    "\n",
    "    #filter with cloud free threshold to remove cloudy scenes\n",
    "    mostly_cloud_free = cloud_free.mean(dim=('x', 'y')) >= cloud_free_threshold\n",
    "\n",
    "    # only those observations that were mostly cloud free\n",
    "    result = fc.where(mostly_cloud_free).dropna(dim='time', how='all')\n",
    "    result.attrs['crs'] = crs\n",
    "    result.attrs['affine'] = affine\n",
    "    return result\n",
    "        \n",
    "    print ('complete')\n",
    "\n",
    "    \n",
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "def rasterize_vector(input_data, cols, rows, geo_transform,\n",
    "                     projection, field=None, raster_path=None):\n",
    "    \"\"\"\n",
    "    Rasterize a vector file and return an array with values for cells that occur within the shapefile. \n",
    "    Can be used to obtain a binary array (shapefile vs no shapefile), or can assign the array cells with\n",
    "    values from the shaepfile features by supplying the name of a shapefile field ('field). If 'raster_path' \n",
    "    is provided, the resulting array can be output as a geotiff raster.\n",
    "    \n",
    "    This function requires dimensions, projection data (in \"WKT\" format) and geotransform info \n",
    "    (\"(upleft_x, x_size, x_rotation, upleft_y, y_rotation, y_size)\") for the output array. \n",
    "    These are typically obtained from an existing raster using the following GDAL calls:\n",
    "    \n",
    "    # import gdal\n",
    "    # gdal_dataset = gdal.Open(raster_path)\n",
    "    # geotrans = gdal_dataset.GetGeoTransform()\n",
    "    # prj = gdal_dataset.GetProjection()\n",
    "    # out_array = gdal_dataset.GetRasterBand(1).ReadAsArray() \n",
    "    # yrows, xcols = out_array.shape\n",
    "    \n",
    "    Last modified: April 2018\n",
    "    Author: Robbi Bishop-Taylor\n",
    "\n",
    "    :attr input_data: input shapefile path or preloaded GDAL/OGR layer\n",
    "    :attr cols: desired width of output array in columns. This can be obtained from an existing\n",
    "                array using '.shape[0]')\n",
    "    :attr rows: desired height of output array in rows. This can be obtained from an existing\n",
    "                array using '.shape[1]')\n",
    "    :attr geo_transform: geotransform for output raster; \n",
    "                 e.g. \"(upleft_x, x_size, x_rotation, upleft_y, y_rotation, y_size)\"\n",
    "    :attr projection: projection for output raster (in \"WKT\" format)\n",
    "    :attr field: shapefile field to rasterize values from. If none given (default), this \n",
    "                 assigns a value of 1 to all array cells within the shapefile, and 0 to \n",
    "                 areas outside the shapefile\n",
    "\n",
    "    :returns: a 'row x col' array containing values from vector\n",
    "    \"\"\"\n",
    "\n",
    "    # If input data is a string, import as shapefile layer\n",
    "    if isinstance(input_data, str):\n",
    "        # Open vector with gdal\n",
    "        data_source = gdal.OpenEx(input_data, gdal.OF_VECTOR)\n",
    "        input_data = data_source.GetLayer(0)\n",
    "\n",
    "    # If raster path supplied, save rasterized file as a geotiff\n",
    "    if raster_path:\n",
    "\n",
    "        # Set up output raster\n",
    "        print('Exporting raster to {}'.format(raster_path))\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        target_ds = driver.Create(raster_path, cols, rows, 1, gdal.GDT_UInt16)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # If no raster path, create raster as memory object\n",
    "        driver = gdal.GetDriverByName('MEM')  # In memory dataset\n",
    "        target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "\n",
    "    # Set geotransform and projection\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "\n",
    "    # Rasterize shapefile and extract array using field if supplied; else produce binary array\n",
    "    if field:\n",
    "        gdal.RasterizeLayer(target_ds, [1], input_data, options=[\"ATTRIBUTE=\" + field])\n",
    "    else:\n",
    "        gdal.RasterizeLayer(target_ds, [1], input_data)    \n",
    "    \n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    out_array = band.ReadAsArray()\n",
    "    target_ds = None\n",
    "\n",
    "    return out_array\n",
    "\n",
    "\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def three_band_image(ds, bands, time = 0, figsize = [10, 10], contrast_enhance = False,\n",
    "                     title = 'Time'):\n",
    "    \"\"\"\n",
    "    threeBandImage takes three spectral bands and plots them on the RGB bands of an \n",
    "    image. \n",
    "    \n",
    "    Last modified: March 2018\n",
    "    Author: Mike Barnes\n",
    "    Modified by: Claire Krause, Cate Kooymans\n",
    "    Inputs: \n",
    "    ds -   Dataset containing the bands to be plotted\n",
    "    bands - list of three bands to be plotted\n",
    "    \n",
    "    Optional:\n",
    "    time - Index value of the time dimension of ds to be plotted\n",
    "    figsize - dimensions for the output figure\n",
    "    contrast_enhance - determines the transformation for plotting onto RGB. If contrast_enhance = true, \n",
    "                       exposure.equalize_hist is used to trasnform the data. Else, the data are\n",
    "                       standardised relative to reflectance = 5000.\n",
    "    title - string for the plot title. If nothing is given, it will print the names of the\n",
    "            bands being plotted.\n",
    "    projection - options are 'projected' or 'geographic'. To determine if the image is\n",
    "                in degrees or northings\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        t, y, x = ds[bands[0]].shape\n",
    "        rawimg = np.zeros((y, x, 3), dtype = np.float32)\n",
    "        for i, colour in enumerate(bands):\n",
    "            rawimg[:, :, i] = ds[colour][time].values\n",
    "    except ValueError:\n",
    "        y, x = ds[bands[0]].shape\n",
    "        rawimg = np.zeros((y, x, 3), dtype = np.float32)\n",
    "        for i, colour in enumerate(bands):\n",
    "            rawimg[:, :, i] = ds[colour].values\n",
    "    rawimg[rawimg == -999] = np.nan\n",
    "    if contrast_enhance is True:\n",
    "        img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))\n",
    "    else:\n",
    "        img_toshow = rawimg\n",
    "    fig = plt.figure(figsize = figsize)\n",
    "    plt.imshow(img_toshow, extent = [ds.coords['x'].min(), ds.coords['x'].max(),\n",
    "                          ds.coords['y'].min(), ds.coords['y'].max()])\n",
    "    ax = plt.gca()\n",
    "    if title == 'Time':\n",
    "        try:\n",
    "            ax.set_title(str(ds.time[time].values), fontweight = 'bold', fontsize = 16)\n",
    "        except:\n",
    "            ax.set_title('', fontweight = 'bold', fontsize = 16)\n",
    "    else:\n",
    "        ax.set_title(title, fontweight = 'bold', fontsize = 16)\n",
    "    return plt, fig\n",
    "\n",
    "\n",
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "def array_to_geotiff(fname, data, geo_transform, projection,\n",
    "                     nodata_val=0, dtype=gdal.GDT_Float32):\n",
    "    \"\"\"\n",
    "    Create a single band GeoTIFF file with data from an array. \n",
    "    \n",
    "    Because this works with simple arrays rather than xarray datasets from DEA, it requires\n",
    "    geotransform info (\"(upleft_x, x_size, x_rotation, upleft_y, y_rotation, y_size)\") and \n",
    "    projection data (in \"WKT\" format) for the output raster. These are typically obtained from \n",
    "    an existing raster using the following GDAL calls:\n",
    "    \n",
    "    # import gdal\n",
    "    # gdal_dataset = gdal.Open(raster_path)\n",
    "    # geotrans = gdal_dataset.GetGeoTransform()\n",
    "    # prj = gdal_dataset.GetProjection()\n",
    "    \n",
    "    ...or alternatively, directly from an xarray dataset:\n",
    "    \n",
    "    # geotrans = xarraydataset.geobox.transform.to_gdal()\n",
    "    # prj = xarraydataset.geobox.crs.wkt\n",
    "    \n",
    "    Last modified: March 2018\n",
    "    Author: Robbi Bishop-Taylor\n",
    "    \n",
    "    :attr fname: output file path\n",
    "    :attr data: input array\n",
    "    :attr geo_transform: geotransform for output raster; \n",
    "    \t\t\t e.g. \"(upleft_x, x_size, x_rotation, upleft_y, y_rotation, y_size)\"\n",
    "    :attr projection: projection for output raster (in \"WKT\" format)\n",
    "    :attr nodata_val: value to convert to nodata in output raster; default 0\n",
    "    :attr dtype: value to convert to nodata in output raster; default gdal.GDT_Float32\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up driver\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    # Create raster of given size and projection\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, dtype)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "\n",
    "    # Write data to array and set nodata values\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    band.SetNoDataValue(nodata_val)\n",
    "\n",
    "    # Close file\n",
    "    dataset = None\n",
    "\n",
    "    \n",
    "import gdal\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "\n",
    "def dataset_to_geotiff(filename, data):\n",
    "    kwargs = {'driver': 'GTiff',\n",
    "              'count': len(data.data_vars),  # geomedian no time dim\n",
    "              'width': data.sizes['x'], 'height': data.sizes['y'],\n",
    "              'crs': data.crs.crs_str,\n",
    "              'transform': data.affine,\n",
    "              'dtype': list(data.data_vars.values())[0].values.dtype,\n",
    "              'nodata': 0,\n",
    "              'compress': 'deflate', 'zlevel': 4, 'predictor': 3}\n",
    "    with rasterio.open(filename, 'w', **kwargs) as src:\n",
    "        for i, band in enumerate(data.data_vars):\n",
    "            src.write(data[band].data, i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 61,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Pixel-wise trend and variability analysis\n",
    "<a id=\"pixelWiseAnalysis\"> </a>\n",
    "Functioning code, nothing should need to be changed from here onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fractional Cover dataset from the DataCube\n",
    "<a id=\"get_data\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T00:16:13.104086Z",
     "start_time": "2018-06-01T00:16:13.089907Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#create subdirectories in the results folder for all the outputs\n",
    "#folder in results labelled by the project_ID\n",
    "os.makedirs(results+ project_ID)\n",
    "#netcdfs\n",
    "os.makedirs('results/' + project_ID + '/netcdfs')\n",
    "#tiffs\n",
    "os.makedirs('results/' + project_ID + '/geotiffs')\n",
    "#pdfs (seperate folder for the timeseries plots)\n",
    "os.makedirs('results/' + project_ID + '/pdfs')\n",
    "os.makedirs('results/' + project_ID + '/pdfs/ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T00:16:13.112217Z",
     "start_time": "2018-06-01T00:16:13.106519Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#assign the results directories as variables\n",
    "#netcdfs\n",
    "results_netcdf = results + project_ID + '/netcdfs/'\n",
    "#tiffs\n",
    "tiff_results = results + project_ID + '/geotiffs/'\n",
    "#pdfs (seperate folder for the timeseries plots)\n",
    "results_pdf = results + project_ID + '/pdfs/'\n",
    "results_ts_pdf = results + project_ID + '/pdfs/ts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T00:16:13.530257Z",
     "start_time": "2018-06-01T00:16:13.114757Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#Create a datacube query\n",
    "########################\n",
    "\n",
    "#import project area shapefiles\n",
    "project_area = gpd.read_file(shapefile_loc)\n",
    "\n",
    "#convert the shapefile to GDA94 lat-long coords so we can query dc_load using lat long\n",
    "project_area['geometry'] = project_area['geometry'].to_crs(epsg=4283)\n",
    "\n",
    "#find the bounding box that contains all the queried projects\n",
    "x = project_area.total_bounds\n",
    "#longitude\n",
    "ind = [0,2]\n",
    "extent_long = x[ind]  #+ [-0.025, 0.025]\n",
    "extent_long = tuple(extent_long)\n",
    "#latitude\n",
    "ind1 = [1,3]\n",
    "extent_lat = x[ind1] #+ [-0.025, 0.025]\n",
    "extent_lat = tuple(extent_lat)\n",
    "\n",
    "#datacube query is created\n",
    "query = {'time': (start_date, end_date),}\n",
    "query['x'] = extent_long\n",
    "query['y'] = extent_lat\n",
    "query['dask_chunks']= {'time' : ds_chunks} #divide query into chunks to save memory\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:44:47.110178Z",
     "start_time": "2018-06-01T00:16:13.533062Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#Get data using dc.load\n",
    "#######################\n",
    "\n",
    "# Function for extracting the FC data is is a .py file in the 'src' folder\n",
    "# only extracting a single sensor at a time with this function as dask will break if \n",
    "# you try to concatentate multiple sensors together\n",
    "import dask\n",
    "dask.set_options(get=dask.get)\n",
    "\n",
    "fc_clean_ls5 = load_masked_FC('ls5', query, cloud_free_threshold)\n",
    "fc_clean_ls7 = load_masked_FC('ls7', query, cloud_free_threshold)\n",
    "fc_clean_ls8 = load_masked_FC('ls8', query, cloud_free_threshold)\n",
    "\n",
    "#Add a Total Vegetation variable that is the combination of PV and NPV\n",
    "fc_clean_ls5['TV'] = fc_clean_ls5.PV + fc_clean_ls5.NPV\n",
    "fc_clean_ls7['TV'] = fc_clean_ls7.PV + fc_clean_ls7.NPV\n",
    "fc_clean_ls8['TV'] = fc_clean_ls8.PV + fc_clean_ls8.NPV\n",
    "\n",
    "#export out netcdf filed for merging in cdo, use datacube function not xarray function\n",
    "datacube.storage.storage.write_dataset_to_netcdf(fc_clean_ls5, results_netcdf + 'fc_clean_ls5.nc')\n",
    "datacube.storage.storage.write_dataset_to_netcdf(fc_clean_ls7, results_netcdf + 'fc_clean_ls7.nc')\n",
    "datacube.storage.storage.write_dataset_to_netcdf(fc_clean_ls8, results_netcdf + 'fc_clean_ls8.nc')\n",
    "\n",
    "#######################################\n",
    "#Concatenate and clean up the timeseries\n",
    "#######################################\n",
    "\n",
    "#merge the timeseries together using cdo\n",
    "os.chdir(results_netcdf)\n",
    "\n",
    "ifile = 'fc_clean_ls5.nc'\n",
    "ifile1 = 'fc_clean_ls7.nc'\n",
    "ifile2 = 'fc_clean_ls8.nc'\n",
    "ofile = 'fc_clean_allobs.nc'\n",
    "cdo.mergetime(input=\" \".join((ifile,ifile1,ifile2)), output=ofile)\n",
    "\n",
    "os.chdir(working_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Masking\n",
    "<a id=\"masking\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:44:49.291480Z",
     "start_time": "2018-06-01T02:44:47.114266Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 65,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#---------Masking--------\n",
    "#########################\n",
    "#reimport netcdf file for masking operation\n",
    "fc_clean_allobs = xr.open_dataset(results_netcdf + 'fc_clean_allobs.nc').chunk({'time':20})\n",
    "\n",
    "#reload, reproject, and export the shapefile (so we can use it for masking in the next step)\n",
    "#load\n",
    "project_area_CEA = gpd.read_file(shapefile_loc)\n",
    "#convert the shapefile to albers equal area coords\n",
    "project_area_CEA = project_area_CEA.to_crs(epsg=3577)\n",
    "#export out converted shapefile\n",
    "project_area_CEA.to_file(data + project_ID + '_cea_albers.shp')\n",
    "\n",
    "#Rasterize shapefile vector for masking:\n",
    "#find the 'wkt' for GDA94 Albers Equal Area\n",
    "import osr\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(3577)\n",
    "prj_wkt = srs.ExportToWkt()\n",
    "\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = fc_clean_allobs.TV[1].shape\n",
    "\n",
    "#define the location of the shapefile we're converting\n",
    "project_area_CEA_albers_loc = data + project_ID + '_cea_albers.shp'\n",
    "\n",
    "#create 'transform' tuple that will define the dimensions of the rasterized shapefile\n",
    "easting = float(fc_clean_allobs.x[0])\n",
    "W_E_pixelRes = float(fc_clean_allobs.y[0] - fc_clean_allobs.y[1])\n",
    "rotation = 0.0 #(if image is 'north up')\n",
    "northing = float(fc_clean_allobs.y[0])\n",
    "rotation1 = 0.0 #(if image is 'north up')\n",
    "N_S_pixelRes = float(fc_clean_allobs.x[0] - fc_clean_allobs.x[1])\n",
    "\n",
    "transform = (easting, W_E_pixelRes, rotation, northing, rotation1, N_S_pixelRes)\n",
    "\n",
    "# rasterize vector\n",
    "project_cea_raster = rasterize_vector(project_area_CEA_albers_loc, height, width, transform,\n",
    "                     prj_wkt, raster_path=None)\n",
    "# Mask the xarray\n",
    "fc_clean_masked = fc_clean_allobs.where(project_cea_raster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CDO operations\n",
    "<a id=\"CDO_operations\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:50:50.053454Z",
     "start_time": "2018-06-01T02:44:49.294155Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Trend and Variability analysis\n",
    "################################\n",
    "\n",
    "# resample to the monthly scale so we have a equal interval timeseries for the trend analysis\n",
    "# This also helps reduce the size of the dataset which speeds up operations. But it also reduces the\n",
    "# number of datapoints for the variability analysis, so there's a tradeoff here.\n",
    "fc_clean_monthly = fc_clean_masked.resample('M', skipna=True, how='mean', dim='time')\n",
    "#export out netcdf for cdo operations\n",
    "fc_clean_monthly.to_netcdf(results_netcdf + 'fc_clean_monthly.nc',format='NETCDF4') \n",
    "\n",
    "#use CDO to do the pixel-wise analysis\n",
    "os.chdir(results_netcdf)                                                      \n",
    "#long term trends and variability\n",
    "cdo.settaxis(start_date[:7]+'-16,12:00:00,1mon', input = 'fc_clean_monthly.nc', output = 'fc_ready.nc', options='-r') #fix the time axis\n",
    "cdo.selyear(start_of_baseline+'/'+end_of_baseline, input='fc_ready.nc', output=start_of_baseline+'_'+end_of_baseline+'.nc')\n",
    "cdo.timmean(input = start_of_baseline+'_'+end_of_baseline+'.nc', output = 'mean'+start_of_baseline+'_'+end_of_baseline+'.nc')                                \n",
    "cdo.timstd(input = start_of_baseline+'_'+end_of_baseline+'.nc', output = 'std'+start_of_baseline+'_'+end_of_baseline+'.nc')    \n",
    "cdo.trend(input=start_of_baseline+'_'+end_of_baseline+'.nc', output = 'intercept'+start_of_baseline+'_'+end_of_baseline+'.nc'+ \" \" 'slope'+start_of_baseline+'_'+end_of_baseline+'.nc')    #calculate pixel-wise linear coefficients and intercepts\n",
    "\n",
    "#recent trends and variability\n",
    "cdo.selyear(start_of_regen+'/'+end_date[0:4], input='fc_ready.nc', output=start_of_regen+'_'+end_date[0:4]+'.nc')\n",
    "cdo.timmean(input = start_of_regen+'_'+end_date[0:4]+'.nc', output = 'mean'+start_of_regen+'_'+end_date[0:4]+'.nc')                                 \n",
    "cdo.timstd(input = start_of_regen+'_'+end_date[0:4]+'.nc', output = 'std'+start_of_regen+'_'+end_date[0:4]+'.nc') \n",
    "cdo.trend(input=start_of_regen+'_'+end_date[0:4]+'.nc', output = 'intercept'+start_of_regen+'_'+end_date[0:4]+'.nc' + \" \" 'slope'+start_of_regen+'_'+end_date[0:4]+'.nc')\n",
    "\n",
    "#differnce in the means between the two periods\n",
    "cdo.sub(input='mean'+start_of_regen+'_'+end_date[0:4]+'.nc' +\" \"+ 'mean'+start_of_baseline+'_'+end_of_baseline+'.nc', output = 'mean_diff.nc') \n",
    "\n",
    "#---------further analysis (not necessarily using all this data)----------\n",
    "#inter AND intra-annual variability test\n",
    "cdo.sub(input = 'std'+start_of_baseline+'_'+end_of_baseline+'.nc'+\" \"+ 'std'+start_of_regen+'_'+end_date[0:4]+'.nc', output = 'std_diff.nc')                   \n",
    "cdo.expr('\"noRegen=(TV<=0)\"', input = 'std_diff.nc', output = 'noRegen_interannual.nc') \n",
    "\n",
    "#intra-annual variability only test\n",
    "#long-term\n",
    "cdo.yearstd(input=start_of_baseline+'_'+end_of_baseline+'.nc', output='yearly_std_baseline.nc')\n",
    "cdo.timmean(input='yearly_std_baseline.nc', output = 'mean_yearly_std_baseline.nc')             \n",
    "#recent intraannual std:\n",
    "cdo.yearstd(input=start_of_regen+'_'+end_date[0:4]+'.nc', output='yearly_std_regen_period.nc')\n",
    "cdo.timmean(input='yearly_std_regen_period.nc', output = 'mean_yearly_std_regen_period.nc')  \n",
    "cdo.sub(input='mean_yearly_std_baseline.nc mean_yearly_std_regen_period.nc', output = 'mean_yearly_std_diff.nc') \n",
    "cdo.expr('\"noRegen=(TV<=0)\"', input = 'mean_yearly_std_diff.nc', output = 'noRegen_intraannual.nc')\n",
    "         \n",
    "os.chdir(working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 65,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "## Statistical tests (Welch-T and Levene)\n",
    "<a id=\"welch-test\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:50:50.529515Z",
     "start_time": "2018-06-01T02:50:50.057554Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#bring in data\n",
    "baseline = xr.open_dataset(results_netcdf +start_of_baseline+'_'+end_of_baseline+'.nc', decode_times=False)\n",
    "baseline = baseline.TV\n",
    "\n",
    "recent = xr.open_dataset(results_netcdf + start_of_regen+'_'+end_date[0:4]+'.nc', decode_times=False)\n",
    "recent = recent.TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:57:04.601095Z",
     "start_time": "2018-06-01T02:50:50.535790Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define a function for calculating the t-stat\n",
    "from scipy import stats\n",
    "def t_test_grid(xarray_a, xarray_b, equal_variance = False, nan_policy= 'omit', mask_not_sig = True, level_of_sig = 0.05, center='mean'):\n",
    "    \"\"\"\n",
    "    This function has two components:\n",
    "    1.  A two-sided T-test for the null hypothesis that 2 independent samples have identical average (expected) values. \n",
    "        This test assumes that the populations have unequal variances by default.\n",
    "    \n",
    "    2. Levene Stat tests the null hypothesis that all input samples are from populations with equal variances.\n",
    "    \n",
    "    See scipy.stats.ttest_ind and scipy.stats.levene for info on the variables in the function\n",
    "    \"\"\"\n",
    "    #convert into numpy ndarray arrays\n",
    "    arr_1 = xarray_a.values\n",
    "    arr_2 = xarray_b.values\n",
    "    #run the t-test\n",
    "    print('starting T-test')\n",
    "    t_stat, p_values = stats.ttest_ind(arr_1, arr_2, equal_var = equal_variance, nan_policy = nan_policy)\n",
    "    print('finished T-test')\n",
    "    \n",
    "    #create empty arrays to put the F-stat results in\n",
    "    t1, x1, y1 = arr_1.shape\n",
    "    t2, x2, y2 = arr_2.shape\n",
    "    assert x1 == x2 and y1 == y2\n",
    "    levene_f = np.zeros((x1, y1))\n",
    "    levene_p = np.zeros((x1, y1))\n",
    "    #loop through each cell of arr1 and arr2 to conduct the levene test\n",
    "    print('Starting for-loop...I am thinking')\n",
    "    for x in range(x1):\n",
    "        for y in range(y1):\n",
    "            arr_3 = arr_1[:, x, y] #for each x,y position, create a 1D array of the timeseries\n",
    "            arr_4 = arr_2[:, x, y]\n",
    "            arr_3 = arr_3[~np.isnan(arr_3)] #deal with the nans\n",
    "            arr_4 = arr_4[~np.isnan(arr_4)]\n",
    "            levene_f[x,y], levene_p[x,y] = stats.levene(arr_3, arr_4, center=center) #run the test     \n",
    "    print('finished levene test')\n",
    "\n",
    "    #Get coordinates from the original xarray\n",
    "    lat  = xarray_a.coords['y']\n",
    "    long = xarray_a.coords['x']\n",
    "    #Mask out values with insignificant trends (ie. p-value > 0.05) if user wants\n",
    "    if mask_not_sig == True:\n",
    "        t_stat[p_values>level_of_sig]=np.nan        \n",
    "        levene_f[levene_p>level_of_sig]=np.nan\n",
    "    #Write arrays into a x-array\n",
    "    t_stat_xr = xr.DataArray(t_stat, coords = [lat, long], dims = ['y', 'x'], name='t_stats')\n",
    "    p_val_xr = xr.DataArray(p_values, coords = [lat, long], dims = ['y', 'x'], name='p_value') \n",
    "    f_stat_xr = xr.DataArray(levene_f, coords = [lat, long], dims = ['y', 'x'], name='f_stats')\n",
    "    p_val_f_xr = xr.DataArray(levene_p, coords = [lat, long], dims = ['y', 'x'], name='p_value_f') \n",
    "    return t_stat_xr, p_val_xr, f_stat_xr, p_val_f_xr\n",
    "\n",
    "#run function\n",
    "t_stat_masked, p_values, levene_stat_masked, p_values_levene = t_test_grid(baseline, recent)\n",
    "t_stat, p_values, levene_stat, p_values_levene = t_test_grid(baseline, recent, mask_not_sig = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:57:04.917289Z",
     "start_time": "2018-06-01T02:57:04.603814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export out results as netcdf\n",
    "t_stat.to_netcdf(results_netcdf + 't_stat.nc', mode='w',format='NETCDF4') \n",
    "t_stat_masked.to_netcdf(results_netcdf + 't_stat_masked.nc', mode='w',format='NETCDF4') \n",
    "\n",
    "p_values.to_netcdf(results_netcdf + 'p_value.nc', mode='w',format='NETCDF4') \n",
    "\n",
    "levene_stat.to_netcdf(results_netcdf + 'levene_stat.nc', mode='w',format='NETCDF4')\n",
    "levene_stat_masked.to_netcdf(results_netcdf + 'levene_stat_masked.nc', mode='w',format='NETCDF4')\n",
    "p_values_levene.to_netcdf(results_netcdf + 'p_values_levene.nc', mode='w',format='NETCDF4') \n",
    "\n",
    "#export out a -log(p_value) netcdf - we'll use it in the next step as it's easier to inpterpet than the 0-1 p_values.\n",
    "negLog_pValues = -np.log(p_values)\n",
    "negLog_pValues.to_netcdf(results_netcdf + 'negLog_Pvalue.nc', mode='w',format='NETCDF4')\n",
    "\n",
    "negLog_pValues_levene = -np.log(p_values_levene)\n",
    "negLog_pValues_levene.to_netcdf(results_netcdf + 'negLog_Pvalue_levene.nc', mode='w',format='NETCDF4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CompositeRGB plot\n",
    "<a id=\"rgb_plot\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:57:05.711634Z",
     "start_time": "2018-06-01T02:57:04.919783Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create an RGB xarray/netcdf that combines the p-values, the absolute difference in the means between periods, \n",
    "# and the difference in standard deviation between the periods to create a single file that detects change\n",
    "mean_diff = xr.open_dataset(results_netcdf + 'mean_diff.nc', decode_times=False)\n",
    "mean_diff = mean_diff.TV.squeeze(dim='time')\n",
    "mean_diff = mean_diff.drop('time')\n",
    "\n",
    "mean_yearly_std_diff = xr.open_dataset(results_netcdf + 'mean_yearly_std_diff.nc', decode_times=False)\n",
    "mean_yearly_std_diff = mean_yearly_std_diff.TV.squeeze(dim='time')\n",
    "mean_yearly_std_diff = mean_yearly_std_diff.drop('time')\n",
    "\n",
    "#create rgb xarray object\n",
    "from datacube.utils.geometry import CRS\n",
    "from affine import Affine\n",
    "x = {'mean_diff':mean_diff, 'std_diff': mean_yearly_std_diff, 'negLog_pValue': negLog_pValues}\n",
    "rgb_changeDetect = xr.Dataset(x, attrs={'crs' : CRS('EPSG:3577'),'affine': fc_clean_ls8.affine })\n",
    "\n",
    "#export as a netcdf and a geotiff\n",
    "datacube.storage.storage.write_dataset_to_netcdf(rgb_changeDetect, results_netcdf + 'rgb_changeDetect.nc')\n",
    "dataset_to_geotiff(tiff_results + 'b_rgb_changeDetect.tif',rgb_changeDetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 65,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "## Exporting geotiffs\n",
    "<a id=\"export_geotiffs\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:57:05.975367Z",
     "start_time": "2018-06-01T02:57:05.714392Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Import and prepare data for exporting/plotting\n",
    "###############################################\n",
    "\n",
    "#Import 'No Regeneration' and slope netdcfs files as an xarray:\n",
    "no_regen_intraannual = xr.open_dataset(results_netcdf + 'noRegen_intraannual.nc', decode_times=False)\n",
    "no_regen_intraannual = no_regen_intraannual.drop(labels = 'time_bnds')\n",
    "no_regen_intraannual = no_regen_intraannual.to_array()\n",
    "no_regen_intraannual = no_regen_intraannual.squeeze(dim=('time','variable')).values\n",
    "\n",
    "mean_yearly_std_diff = xr.open_dataset(results_netcdf + 'mean_yearly_std_diff.nc', decode_times=False)\n",
    "mean_yearly_std_diff = mean_yearly_std_diff.TV\n",
    "mean_yearly_std_diff = mean_yearly_std_diff.squeeze(dim='time').values\n",
    "\n",
    "#Import slope here\n",
    "slope_recent = xr.open_dataset(results_netcdf + 'slope'+start_of_regen+'_'+end_date[0:4]+'.nc', decode_times=False)\n",
    "slope_recent = slope_recent.TV\n",
    "slope_recent = slope_recent.squeeze(dim='time').values\n",
    "\n",
    "#import t_stat\n",
    "t_stat_masked_np = xr.open_dataset(results_netcdf + 't_stat_masked.nc')\n",
    "t_stat_masked_np =  t_stat_masked_np.t_stats.values\n",
    "\n",
    "t_stat_np = xr.open_dataset(results_netcdf + 't_stat.nc')\n",
    "t_stat_np =  t_stat_np.t_stats.values\n",
    "\n",
    "#import p_value\n",
    "negLog_Pvalue_np = xr.open_dataset(results_netcdf + 'p_value.nc')\n",
    "negLog_Pvalue_np =  negLog_Pvalue_np.p_value.values\n",
    "\n",
    "#import levene stats\n",
    "levene_stat_masked_np = xr.open_dataset(results_netcdf + 'levene_stat_masked.nc')\n",
    "levene_stat_masked_np =  levene_stat_masked_np.f_stats.values\n",
    "\n",
    "levene_stat_np = xr.open_dataset(results_netcdf + 'levene_stat.nc')\n",
    "levene_stat_np =  levene_stat_np.f_stats.values\n",
    "\n",
    "#import levene p_values\n",
    "negLog_Pvalue_levene_np = xr.open_dataset(results_netcdf + 'negLog_Pvalue_levene.nc')\n",
    "negLog_Pvalue_levene_np =  negLog_Pvalue_levene_np.p_value_f.values\n",
    "\n",
    "#import recent_mean minus baseline_mean\n",
    "mean_diff_np = xr.open_dataset(results_netcdf + 'mean_diff.nc', decode_times=False)\n",
    "mean_diff_np = mean_diff_np.TV.squeeze(dim='time').values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T02:57:06.362557Z",
     "start_time": "2018-06-01T02:57:05.978023Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export the masked xarrays as geotiffs. Transform object is being defined above\n",
    "array_to_geotiff(fname=tiff_results + 'no_regen_intraannual_masked.tif', \n",
    "                                    data = no_regen_intraannual,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt, \n",
    "                                    nodata_val=np.nan)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 'mean_yearly_std_diff.tif',\n",
    "                                    data = mean_yearly_std_diff,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt, \n",
    "                                    nodata_val=np.nan)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 'slope_recent.tif',\n",
    "                                    data = slope_recent,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt, \n",
    "                                    nodata_val=np.nan)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 't_stat_masked.tif',\n",
    "                                    data = t_stat_masked_np,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt,\n",
    "                                    nodata_val=np.nan, dtype=gdal.GDT_Float64)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 't_stat.tif',\n",
    "                                    data = t_stat_np,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt,\n",
    "                                    nodata_val=np.nan, dtype=gdal.GDT_Float64)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 'negLog_p_value.tif',\n",
    "                                    data = negLog_Pvalue_np,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt,\n",
    "                                    nodata_val=np.nan, dtype=gdal.GDT_Float64)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 'levene_stat_masked.tif',\n",
    "                                    data = levene_stat_masked_np,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt,\n",
    "                                    nodata_val=np.nan, dtype=gdal.GDT_Float64)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 'levene_stat.tif',\n",
    "                                    data = levene_stat_np,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt,\n",
    "                                    nodata_val=np.nan, dtype=gdal.GDT_Float64)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 'negLog_p_value_levene.tif',\n",
    "                                    data = negLog_Pvalue_levene_np,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt,\n",
    "                                    nodata_val=np.nan, dtype=gdal.GDT_Float64)\n",
    "\n",
    "array_to_geotiff(fname=tiff_results + 'mean_difference.tif',\n",
    "                                    data = mean_diff_np,\n",
    "                                    geo_transform = transform, \n",
    "                                    projection = prj_wkt,\n",
    "                                    nodata_val=np.nan, dtype=gdal.GDT_Float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 69,
        "width": 12
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "# Zonal Statistics\n",
    "<a id=\"zonalStats\"> </a>\n",
    "Use the exported 'fc_clean_monthly.nc' file to calculate the zonal statistics over polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:00:33.553058Z",
     "start_time": "2018-06-01T02:57:06.364941Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import the fractional cover netcdf file\n",
    "fc_monthly = xr.open_dataset(results_netcdf + \"fc_clean_monthly.nc\", chunks = {'time':ds_chunks})\n",
    "\n",
    "#select out just the TV band\n",
    "TV_array = fc_monthly.TV.chunk(chunks = {'time':ds_chunks})\n",
    "\n",
    "#create 'transform' tuple to provide ndarray with geo-referencing data. \n",
    "#The tuple contains 6 elements which correspond to:\n",
    "easting = float(TV_array.x[0])\n",
    "W_E_pixelRes = float(TV_array.y[0] - TV_array.y[1])\n",
    "rotation = 0.0 #(if image is 'north up')\n",
    "northing = float(TV_array.y[0])\n",
    "rotation1 = 0.0 #(if image is 'north up')\n",
    "N_S_pixelRes = float(TV_array.x[0] - TV_array.x[1])\n",
    "\n",
    "transform_zonal = (easting, W_E_pixelRes, rotation, northing, rotation1, N_S_pixelRes)\n",
    "\n",
    "#set the index of the geopandas datframe to be the Feature Name\n",
    "project_area_CEA = project_area_CEA.set_index(feature_name)\n",
    "\n",
    "# create the zonal stats functions\n",
    "import rasterstats as rs\n",
    "def zonalStats_mean(chunk): \n",
    "    \"\"\"extract the zonal mean of all\n",
    "    pixel values within each polygon\"\"\"\n",
    "    mean = [] \n",
    "    for i in chunk:\n",
    "        x = rs.zonal_stats(project_area_CEA, i, transform=transform_zonal, stats='mean')    \n",
    "        mean.append(x)\n",
    "    #extract just the values from the results, and convert 'None' values to nan\n",
    "    mean = [[t['mean'] if t['mean'] is not None else np.nan for t in TV] for TV in mean]\n",
    "    mean = np.array(mean)\n",
    "    return mean\n",
    "\n",
    "def zonalStats_std(chunk): \n",
    "    \"\"\"extract the standard deviation\n",
    "    of the pixel values within the polygon\"\"\"\n",
    "    std = [] \n",
    "    for i in chunk:\n",
    "        x = rs.zonal_stats(project_area_CEA, i, transform=transform_zonal, stats='std')    \n",
    "        std.append(x)\n",
    "    std = [[t['std'] if t['std'] is not None else np.nan for t in TV] for TV in std]\n",
    "    std = np.array(std)\n",
    "    return std\n",
    "\n",
    "#use the zonal_stats functions to extract the mean and std from each sensor:\n",
    "n = len(project_area_CEA) #number of polygons in the shapefile (defines the dimesions of the output)\n",
    "#calculate zonal stats\n",
    "TV_mean = TV_array.data.map_blocks(zonalStats_mean, chunks=(-1,n), drop_axis=1, dtype=np.float64).compute()\n",
    "TV_std = TV_array.data.map_blocks(zonalStats_std, chunks=(-1,n), drop_axis=1, dtype=np.float64).compute()\n",
    "\n",
    "#get unique identifier (project ids) and timeseries data from the inputs \n",
    "colnames = pd.Series(project_area_CEA.index.values)\n",
    "time = pd.Series(fc_monthly['time'].values)\n",
    "\n",
    "#define functions for cleaning up the results of the rasterstats operation\n",
    "def tidyresults(results):\n",
    "    \"\"\"take the results of the zonal_stats function\n",
    "    and place into a sensibly indexed dataframe\"\"\"\n",
    "    x = pd.DataFrame(results).T #transpose\n",
    "    x = x.rename(colnames, axis='index') #rename the columns to the timestamp\n",
    "    x = x.rename(columns = time)\n",
    "    #x = x.replace(x[x<=0], np.nan)\n",
    "    return x\n",
    "\n",
    "#place results into indexed dataframes using tidyresults function\n",
    "TV_mean_df = tidyresults(TV_mean)\n",
    "TV_std_df = tidyresults(TV_std)\n",
    "\n",
    "#convert mean and std dfs into xarray for merging into a dataset\n",
    "TV_mean_xr = xr.DataArray(TV_mean_df, dims=[feature_name, 'time'], coords={feature_name: TV_mean_df.index, 'time': time}, name= 'TV_mean')\n",
    "TV_std_xr = xr.DataArray(TV_std_df, dims=[feature_name, 'time'], coords={feature_name: TV_std_df.index, 'time': time}, name= 'TV_std')\n",
    "\n",
    "#join the mean and std dataArrays together\n",
    "TV_zonalStats = xr.merge([TV_mean_xr, TV_std_xr]) \n",
    "\n",
    "#export out results as netcdf\n",
    "TV_zonalStats.to_netcdf('results/' + project_ID + '/netcdfs/' + 'FC_zonalstats_monthly.nc', mode='w',format='NETCDF4') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 73,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# Reporting and plotting\n",
    "<a id=\"reporting\"> </a>\n",
    "Create a pdf report that contains all of the above analysis in one easy document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 77,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "heading_collapsed": true
   },
   "source": [
    "## Zonal timeseries plots\n",
    "<a id=\"timeseries\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:00:35.975939Z",
     "start_time": "2018-06-01T03:00:33.555640Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 16,
        "hidden": false,
        "row": 77,
        "width": 4
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "#create plots of the zonally averaged TV timeseries\n",
    "###################################################\n",
    "CEA_data_monthly = []\n",
    "for i in range(0,len(TV_zonalStats[feature_name])):\n",
    "    x = TV_zonalStats.isel(['TV_mean','TV_std'], **{feature_name: i})\n",
    "    CEA_data_monthly.append(x)\n",
    "    \n",
    "#extract the unique names of each polygon\n",
    "CEA_monthly = list(TV_zonalStats[feature_name].values)\n",
    "\n",
    "#zip the both the CEAs and CEA_data together as a dictionary \n",
    "monthly_dict = dict(zip(CEA_monthly,CEA_data_monthly))\n",
    "\n",
    "#create a function for generating the plots\n",
    "def plotResults(data, title):\n",
    "    \"\"\"a function for plotting up the results of the\n",
    "    fractional cover change and exporting it out as pdf \"\"\"\n",
    "    x = data.time.values\n",
    "    TV = data.TV_mean\n",
    "    TV_error = data.TV_std\n",
    "\n",
    "    #plot TV\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(x, TV,'k', color='#228b22', linewidth = 1)\n",
    "    plt.fill_between(x, TV-TV_error, TV+TV_error,\n",
    "                          alpha=0.5, edgecolor='#228b22', facecolor='#32cd32')\n",
    "    plt.grid(True, linestyle ='--')\n",
    "    plt.ylabel('Fraction of TV Cover (%)')\n",
    "    plt.ylim(0,100)\n",
    "    plt.title(title + ', TV')\n",
    "\n",
    "    plt.savefig(results_ts_pdf +title+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "#loop over the dictionaries and create the plots\n",
    "{key: plotResults(monthly_dict[key], key + '_monthly') for key in monthly_dict}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:00:36.154880Z",
     "start_time": "2018-06-01T03:00:35.978444Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#merge all the exported timeseries pdfs into one document\n",
    "from PyPDF2 import PdfFileMerger\n",
    "os.chdir(results_ts_pdf) #navigate to directory\n",
    "pdfs = os.listdir() #list all the pdfs in the directory\n",
    "pdfs = sorted(pdfs)\n",
    "\n",
    "#merge the pdfs\n",
    "merger = PdfFileMerger()\n",
    "for pdf in pdfs:\n",
    "    merger.append(pdf)\n",
    "merger.write(\"b_all_timeseries.pdf\")\n",
    "\n",
    "#move the merged pdf into the directory above it (helps with merge of all the results later)\n",
    "os.rename(results_ts_pdf+\"b_all_timeseries.pdf\", results_pdf+'b_all_timeseries.pdf')\n",
    "os.chdir(working_directory) #navigate back to our wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 77,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Pixel-wise plots\n",
    "<a id=\"pixel-wise_plots\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:00:36.248031Z",
     "start_time": "2018-06-01T03:00:36.157540Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#Create pixel-wise plots\n",
    "########################\n",
    "\n",
    "#import files\n",
    "no_regen_intraannual = xr.open_dataset(results_netcdf + 'noRegen_intraannual.nc', decode_times=False)\n",
    "no_regen_intraannual = no_regen_intraannual.drop(labels = 'time_bnds')\n",
    "\n",
    "mean_yearly_std_diff = xr.open_dataset(results_netcdf + 'mean_yearly_std_diff.nc', decode_times=False)\n",
    "mean_yearly_std_diff = mean_yearly_std_diff.TV\n",
    "mean_yearly_std_diff = mean_yearly_std_diff.squeeze('time')\n",
    "\n",
    "mean_diff = xr.open_dataset(results_netcdf + 'mean_diff.nc', decode_times=False)\n",
    "mean_diff = mean_diff.TV\n",
    "mean_diff = mean_diff.squeeze('time')\n",
    "\n",
    "p_value = xr.open_dataset(results_netcdf + 'p_value.nc', decode_times=False)\n",
    "\n",
    "p_value_levene = xr.open_dataset(results_netcdf + 'p_values_levene.nc', decode_times=False)\n",
    "\n",
    "rgb_changeDetect = xr.open_dataset(results_netcdf + 'rgb_changeDetect.nc', decode_times=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:14:07.542779Z",
     "start_time": "2018-06-01T03:00:36.250750Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 81,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#create a plot showing the Std. Dev,. difference and mean difference maps, as well as the P-values for those changes.\n",
    "\n",
    "#generate some x and y limits so the plotting is constrained\n",
    "y = project_area_CEA.total_bounds\n",
    "#longitude\n",
    "ind = [0,2]\n",
    "xlim = y[ind]  + [-1000, 1000]\n",
    "xlim = tuple(xlim)\n",
    "#latitude\n",
    "ind1 = [1,3]\n",
    "ylim = y[ind1] + [-1000, 1000]\n",
    "ylim = tuple(ylim)\n",
    "\n",
    "#plot\n",
    "from matplotlib import cm\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.subplot(221)\n",
    "mean_yearly_std_diff.plot(cmap='RdYlGn', vmin=-3,vmax=3)\n",
    "plt.xlim(xlim)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.ylim(ylim)\n",
    "plt.grid(linestyle='dashed', linewidth=1,alpha=1)\n",
    "plt.tick_params(axis='both', labelbottom=False, labelleft=False)\n",
    "plt.title('Difference in Std. Dev. (baseline - recent)')\n",
    "\n",
    "plt.subplot(222)\n",
    "p_value_levene.p_value_f.plot(cmap='magma')\n",
    "ax=plt.subplot(222)\n",
    "plt.xlim(xlim)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.ylim(ylim)\n",
    "plt.grid(linestyle='dashed', linewidth=1,alpha=1)\n",
    "plt.tick_params(axis='both', labelbottom=False, labelleft=False)\n",
    "plt.title('P values of Levene test')\n",
    "\n",
    "plt.subplot(223)\n",
    "mean_diff.plot(cmap='BrBG', vmin=-10.0,vmax=10.0)\n",
    "plt.xlim(xlim)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.ylim(ylim)\n",
    "plt.grid(linestyle='dashed', linewidth=1,alpha=1)\n",
    "plt.tick_params(axis='both', labelbottom=False, labelleft=False)\n",
    "plt.title('Difference in mean TV (recent - baseline)')\n",
    "\n",
    "plt.subplot(224)\n",
    "p_value.p_value.plot(cmap='magma')\n",
    "ax=plt.subplot(224)\n",
    "plt.xlim(xlim)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.ylim(ylim)\n",
    "plt.grid(linestyle='dashed', linewidth=1,alpha=1)\n",
    "plt.tick_params(axis='both', labelbottom=False, labelleft=False)\n",
    "plt.title('P_values of mean TV')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(results_pdf + \"c_pixel_wise_plots.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:14:07.569326Z",
     "start_time": "2018-06-01T03:14:07.545416Z"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate the number of cells that are at a high-risk of no-regeneration for the std dev method\n",
    "#we want to calculate this for the method where I do take account of the p-values_levene, and the where we don't\n",
    "\n",
    "#Method ingnoring levene stats results\n",
    "ones = (no_regen_intraannual.noRegen.values == 1).sum()\n",
    "no_of_cells = np.count_nonzero(~np.isnan(no_regen_intraannual.noRegen.values))\n",
    "risk_percent = round(ones/no_of_cells*100, 2)\n",
    "risk_percent = risk_percent.astype(str)\n",
    "\n",
    "#Method incorporating levene stats results\n",
    "ones_masked = np.array(no_regen_intraannual.noRegen.where(p_value_levene.p_value_f<=0.10) == 1).sum()\n",
    "risk_percent_masked = round(ones_masked/no_of_cells*100, 2)\n",
    "risk_percent_masked = risk_percent_masked.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:20:20.822418Z",
     "start_time": "2018-06-01T03:14:07.572154Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "t = (risk_percent +\"% of cells are high-risk (red)\")\n",
    "plt.subplot(121)\n",
    "no_regen_intraannual.noRegen.plot(cmap='RdYlGn_r')\n",
    "ax=plt.subplot(121)\n",
    "plt.text(0.025, 0.025, t,\n",
    "        transform = ax.transAxes, \n",
    "        bbox=dict(facecolor='blue', alpha=0.3),\n",
    "        wrap=True)\n",
    "plt.xlim(xlim)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.ylim(ylim)\n",
    "ax.set_axis_off()\n",
    "fig.delaxes(fig.axes[1])\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)\n",
    "plt.grid(linestyle='dashed', linewidth=1,alpha=1)\n",
    "plt.tick_params(axis='both', labelbottom=False, labelleft=False)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('High Risk of No-Regeneration (Std. Dev. method)')\n",
    "\n",
    "t1 = (risk_percent_masked +\"% of cells are high-risk (red)\")\n",
    "plt.subplot(122)\n",
    "no_regen_intraannual.noRegen.where(p_value_levene.p_value_f<=0.10).plot(cmap='RdYlGn_r')\n",
    "ax1=plt.subplot(122)\n",
    "plt.text(0.025, 0.025, t1,\n",
    "        transform = ax1.transAxes, \n",
    "        bbox=dict(facecolor='blue', alpha=0.3),\n",
    "        wrap=True)\n",
    "plt.xlim(xlim)\n",
    "fig.delaxes(fig.axes[2])\n",
    "plt.axis('off')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.ylim(ylim)\n",
    "plt.grid(linestyle='dashed', linewidth=1,alpha=1)\n",
    "plt.tick_params(axis='both', labelbottom=False, labelleft=False)\n",
    "plt.title('masked with levene stats (p=0.1))')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(results_pdf + \"d_no_regeneration_stdDev_method.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:46:02.951467Z",
     "start_time": "2018-06-01T03:46:01.412956Z"
    }
   },
   "outputs": [],
   "source": [
    "#make the final plot that has 3 of the layers combined\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage import exposure\n",
    "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n",
    "                                  AnnotationBbox)\n",
    "\n",
    "x = rgb_changeDetect.to_array().data\n",
    "img = np.transpose(x,[1,2,0]).copy()\n",
    "for b in range(img.shape[2]):\n",
    "    tmp = img[:,:,b]\n",
    "#     pl, pu = np.percentile(tmp[np.isfinite(tmp)], (10., 90.))\n",
    "    mu = np.nanmedian(tmp)\n",
    "    sd = np.nanstd(tmp)\n",
    "    pl, pu = mu - 2.5*sd, mu + 2.5*sd\n",
    "    img[:,:,b] = rescale_intensity(tmp, in_range=(pl, pu), out_range=(0., 1.))\n",
    "\n",
    "xmin= int(rgb_changeDetect.coords['x'].min().values) + 200\n",
    "xmax= int(rgb_changeDetect.coords['x'].max().values) + 200\n",
    "ymin = int(rgb_changeDetect.coords['y'].min().values) + 200\n",
    "ymax= int(rgb_changeDetect.coords['y'].max().values) + 200    \n",
    "    \n",
    "# plt.figure(figsize = (13.0, 13.0))\n",
    "fig, ax = plt.subplots(figsize = (13.0, 13.0))\n",
    "plt.imshow(img, interpolation='nearest',extent = [rgb_changeDetect.coords['x'].min(), rgb_changeDetect.coords['x'].max(),\n",
    "                          rgb_changeDetect.coords['y'].min(), rgb_changeDetect.coords['y'].max()])\n",
    "plt.title('RGB Change Detection. Red=mean_diff, green=StD_diff, blue=p_value')\n",
    "plt.text(xmin,ymin,\"White = strong regen; Blues ='clearing'; Green&Reds = no change; Pink = ambiguous\" , bbox=dict(facecolor='cyan', alpha=0.3)\n",
    "        , horizontalalignment='left', verticalalignment='bottom', wrap=True)\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "ax.set_facecolor('black')\n",
    "#--Add an image to the plot---------------------------------\n",
    "# rgb_tri = plt.imread((data+'rgbtri.png'), format='png')\n",
    "# newax = fig.add_axes([0.745, 0.215, 0.15, 0.15], zorder=2)\n",
    "# newax.imshow(rgb_tri)\n",
    "# newax.axis('off')\n",
    "#-----------------------------------------------------------\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_pdf + \"e_rgb_changeDetect.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 81,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## True colour plot\n",
    "<a id=\"truecolourplot\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:42:16.612703Z",
     "start_time": "2018-06-01T03:42:08.826998Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 85,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#import a recent landsat image to use for a truecolour plot\n",
    "query_new = {'time': ('2018-01-01', '2018-03-28'),}\n",
    "query_new['x'] = extent_long\n",
    "query_new['y'] = extent_lat\n",
    "\n",
    "recent_image = load_clearlandsat(dc=datacube.Datacube(app='loadrecentimage'),\n",
    "                          sensors=['ls8'],\n",
    "                          query=query_new,\n",
    "                          masked_prop=0.95)\n",
    "\n",
    "red = recent_image.red\n",
    "green = recent_image.green\n",
    "blue = recent_image.blue\n",
    "\n",
    "from datacube.utils.geometry import CRS\n",
    "from affine import Affine\n",
    "rgb={'red':red, 'green':green, 'blue':blue}\n",
    "rgb_xr = xr.Dataset(rgb, attrs={'crs' : CRS('EPSG:3577'),'affine': recent_image.affine })\n",
    "\n",
    "#export the truecolour plot as a geotiff\n",
    "dataset_to_geotiff(tiff_results + 'a_trueColour.tif',rgb_xr.isel(time=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:42:22.709225Z",
     "start_time": "2018-06-01T03:42:20.715909Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 10,
        "hidden": false,
        "row": 89,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#create plot using the custom function 'three_band_image'\n",
    "plt, fig = three_band_image(recent_image, time=0, bands=['red', 'green', 'blue'], contrast_enhance=True)    \n",
    "\n",
    "#bring in shapefile\n",
    "import fiona\n",
    "with fiona.open(project_area_CEA_albers_loc, 'r') as shapefile:\n",
    "    features = [feature[\"geometry\"] for feature in shapefile]\n",
    "ax = plt.gca()\n",
    "\n",
    "#create a long list of colors to iterate over in the plot shapefile code\n",
    "from random import randint\n",
    "colors = []\n",
    "for i in range(100):\n",
    "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
    "\n",
    "#plot the shapefile\n",
    "from descartes import PolygonPatch\n",
    "import matplotlib as mpl\n",
    "patches = [PolygonPatch(feature, \n",
    "                        facecolor=\"none\", \n",
    "                        linewidth=1,label='Label') for feature in features]\n",
    "ax.add_collection(mpl.collections.PatchCollection(patches, edgecolor=[color for color in colors], match_original=True))\n",
    "\n",
    "#create a nice, custom legend\n",
    "import matplotlib.patches as mpatches\n",
    "texts = list(project_area_CEA.index)\n",
    "patches = [mpatches.Patch(color=colors[i], label=\"{:s}\".format(texts[i]) ) for i in range(len(texts)) ]\n",
    "plt.legend(handles=patches, loc=9, ncol=2, fontsize='small', fancybox=True, framealpha=0.3)\n",
    "\n",
    "#make it pretty\n",
    "plt.title('Summer-time 2018 True Colour plot of AOI')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.ylim(ylim)\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_pdf + \"a_true_colour.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 93,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Merge all pdfs\n",
    "<a id=\"mergeallpdfs\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:42:28.806932Z",
     "start_time": "2018-06-01T03:42:28.254377Z"
    },
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#merge all the pdfs into a final results document\n",
    "from PyPDF2 import PdfFileMerger\n",
    "os.chdir(results_pdf)\n",
    "\n",
    "#get list of files in the directory that are pdfs\n",
    "pdfs = []\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdfs.append(os.path.join(file))\n",
    "pdfs = sorted(pdfs)\n",
    "\n",
    "#merge pdfs\n",
    "merger = PdfFileMerger()\n",
    "for pdf in pdfs:\n",
    "    merger.append(pdf)\n",
    "\n",
    "merger.write(\"final_results.pdf\")\n",
    "\n",
    "os.chdir(working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
